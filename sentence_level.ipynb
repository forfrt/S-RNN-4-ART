{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80f657-8117-4e31-9a49-16c2bfa5bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path # if you haven't already done so\n",
    "file = Path(os.getcwd()).resolve()\n",
    "parent, root = file.parent, file.parents[1]\n",
    "print(file)\n",
    "# print(parent, root)\n",
    "sys.path.append(str(parent))\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "# # Additionally remove the current file's directory from sys.path\n",
    "# try:\n",
    "#     sys.path.remove(str(parent))\n",
    "# except ValueError: # Already removed\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e56e3e-963b-4d09-8615-df40da12dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_li=[r\"FR_output_features.csv\", r\"IT_output_features.csv\", r\"SK_output_features.csv\"]\n",
    "session_li=[\"main0\", \"main25\", \"main50\", \"main75\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ead4b24d-9f14-47ba-9d54-c13b9ab1a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "05716abf-0ed6-43b1-acdd-a83adaca3a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df[:5]    Unnamed: 0  Mean Pitch   Max Pitch  Mean Intensity  Max Intensity  \\\n",
      "0           0   62.378338  179.198877       69.949677      78.114851   \n",
      "1           1   51.702229  185.547010       69.422789      79.892876   \n",
      "2           2   53.932833  511.778945       69.036319      77.949600   \n",
      "3           3   52.723395  194.008268       69.591178      78.351791   \n",
      "4           4   42.930457  169.834422       67.006694      77.372524   \n",
      "\n",
      "     Jitter   Shimmer        NHR  dyad  speaker    session  sentence  \\\n",
      "0  0.019850  0.060315  18.430881  6162       61  imitation        10   \n",
      "1  0.021415  0.068678  17.854420  6162       61  imitation        11   \n",
      "2  0.024644  0.063827  16.671792  6162       61  imitation        12   \n",
      "3  0.018365  0.054014  19.276305  6162       61  imitation        13   \n",
      "4  0.025257  0.092803  16.287774  6162       61  imitation        14   \n",
      "\n",
      "   duration(s)  size(K)                          file_name  \n",
      "0         7.67   660.94  SK0102_1_imitation555_10_mono.wav  \n",
      "1         8.69   748.97  SK0102_1_imitation555_11_mono.wav  \n",
      "2         9.91   853.28  SK0102_1_imitation555_12_mono.wav  \n",
      "3         6.80   585.49  SK0102_1_imitation555_13_mono.wav  \n",
      "4         7.11   612.45  SK0102_1_imitation555_14_mono.wav  \n"
     ]
    }
   ],
   "source": [
    "session='main0'\n",
    "feature='NHR'\n",
    "input_df=pd.read_csv(input_file_li[0])\n",
    "print(\"input_df[:5]\", input_df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b7129ca7-38db-4177-8c89-2188770d43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 7778\n",
      "speaker, speaker_sent_li 77 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 78 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7172\n",
      "speaker, speaker_sent_li 71 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 72 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6566\n",
      "speaker, speaker_sent_li 65 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 66 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7980\n",
      "speaker, speaker_sent_li 79 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 80 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7374\n",
      "speaker, speaker_sent_li 73 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 74 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6768\n",
      "speaker, speaker_sent_li 67 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 68 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6162\n",
      "speaker, speaker_sent_li 61 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 62 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7576\n",
      "speaker, speaker_sent_li 75 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 76 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6970\n",
      "speaker, speaker_sent_li 69 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 70 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6364\n",
      "speaker, speaker_sent_li 63 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 64 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n"
     ]
    }
   ],
   "source": [
    "session_input_df=input_df[input_df['session']==session]\n",
    "\n",
    "dyad_reg=r\"(\\d{2})(\\d{2})\"\n",
    "dyad_li=list(set(session_input_df['dyad'].tolist()))\n",
    "\n",
    "for dyad in dyad_li:\n",
    "    print(\"processing\", dyad)\n",
    "    match = re.search(dyad_reg, str(dyad))\n",
    "    speaker=int(match.group(1))\n",
    "    partner=int(match.group(2))\n",
    "\n",
    "    session_speaker_df=session_input_df[session_input_df['speaker']==speaker]\n",
    "    session_partner_df=session_input_df[session_input_df['speaker']==partner]\n",
    "\n",
    "    speaker_sent_li=session_speaker_df['sentence'].tolist()\n",
    "    partner_sent_li=session_partner_df['sentence'].tolist()\n",
    "    \n",
    "    speaker_sent_li.sort()\n",
    "    partner_sent_li.sort()\n",
    "    print(\"speaker, speaker_sent_li\", speaker, speaker_sent_li)\n",
    "    print(\"partner, partner_sent_li\", partner, partner_sent_li)\n",
    "\n",
    "    # dialogue_sent_li=speaker_sent_li+partner_sent_li\n",
    "    # dialogue_sent_li.sort()\n",
    "\n",
    "    speaker_adj_promixity=[]\n",
    "    partner_adj_promixity=[]\n",
    "    speaker_other_promixity=[]\n",
    "    partner_other_promixity=[]\n",
    "\n",
    "    for id, (speaker_sent, partner_sent)in enumerate(zip(speaker_sent_li, partner_sent_li)):\n",
    "        # print(\"id, speaker_sent, partner_sent\", id, speaker_sent, partner_sent)\n",
    "        \n",
    "        speaker_feature=session_speaker_df.loc[session_speaker_df['sentence']==speaker_sent][feature].item()\n",
    "        partner_feature=session_partner_df.loc[session_partner_df['sentence']==partner_sent][feature].item()\n",
    "        # print(\"speaker_feature, partner_feature\", speaker_feature, partner_feature)\n",
    "\n",
    "        adj_distance=abs(speaker_feature-partner_feature)\n",
    "\n",
    "        sample_speaker_sent_li=speaker_sent_li.copy()\n",
    "        sample_partner_sent_li=partner_sent_li.copy()\n",
    "        sample_speaker_sent_li.remove(speaker_sent)\n",
    "        sample_partner_sent_li.remove(partner_sent)\n",
    "        \n",
    "        random10_speaker_features=[session_speaker_df.loc[session_speaker_df['sentence']==sent][feature].item() for sent in random.sample(sample_speaker_sent_li, 10)]\n",
    "        random10_partner_features=[session_partner_df.loc[session_partner_df['sentence']==sent][feature].item() for sent in random.sample(sample_partner_sent_li, 10)]\n",
    "        # print(\"random10_speaker_features\", random10_speaker_features)\n",
    "        # print(\"random10_partner_features\", random10_partner_features)\n",
    "        \n",
    "        other_speaker_distance=abs(speaker_feature-sum(random10_partner_features)/len(random10_partner_features))\n",
    "        other_partner_distance=abs(partner_feature-sum(random10_speaker_features)/len(random10_speaker_features))\n",
    "\n",
    "        speaker_adj_promixity.append(adj_distance)\n",
    "        partner_adj_promixity.append(adj_distance)\n",
    "        speaker_other_promixity.append(other_speaker_distance)\n",
    "        partner_other_promixity.append(other_partner_distance)\n",
    "\n",
    "        if id<len(speaker_sent_li)-1:\n",
    "            next_speaker_sent=speaker_sent_li[id+1]\n",
    "            # print(\"id, next_speaker_sent\", id, next_speaker_sent)\n",
    "            \n",
    "            next_speaker_feature=session_speaker_df.loc[session_speaker_df['sentence']==next_speaker_sent][feature].item()\n",
    "            next_adj_distance=abs(partner_feature-next_speaker_feature)\n",
    "            \n",
    "            next_sample_speaker_sent_li=sample_speaker_sent_li.copy()\n",
    "            next_sample_speaker_sent_li.remove(next_speaker_sent)\n",
    "            next_random10_speaker_features=[session_speaker_df.loc[session_speaker_df['sentence']==sent][feature].item() for sent in random.sample(next_sample_speaker_sent_li, 10)]\n",
    "            # print(\"next_random10_speaker_features\", next_random10_speaker_features)\n",
    "\n",
    "            next_other_speaker_distance=abs(next_speaker_feature-sum(random10_partner_features)/len(random10_partner_features))\n",
    "            next_other_partner_distance=abs(partner_feature-sum(next_random10_speaker_features)/len(next_random10_speaker_features))\n",
    "\n",
    "            speaker_adj_promixity.append(next_adj_distance)\n",
    "            partner_adj_promixity.append(next_adj_distance)\n",
    "            speaker_other_promixity.append(next_other_speaker_distance)\n",
    "            partner_other_promixity.append(next_other_partner_distance)\n",
    "        \n",
    "\n",
    "# return speaker_adj_promixity, partner_adj_promixity, speaker_other_promixity, partner_other_promixity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd84735f-bc71-45eb-8368-be431255e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_proximity(input_df, feature, session):\n",
    "    session_input_df=input_df[input_df['session']==session]\n",
    "    \n",
    "    dyad_reg=r\"(\\d{2})(\\d{2})\"\n",
    "    dyad_li=list(set(session_input_df['dyad'].tolist()))\n",
    "    \n",
    "    speaker_adj_promixity_dict={}\n",
    "    partner_adj_promixity_dict={}\n",
    "    speaker_other_promixity_dict={}\n",
    "    partner_other_promixity_dict={}\n",
    "    \n",
    "    for dyad in dyad_li:\n",
    "        print(\"processing\", dyad)\n",
    "        match = re.search(dyad_reg, str(dyad))\n",
    "        speaker=int(match.group(1))\n",
    "        partner=int(match.group(2))\n",
    "    \n",
    "        session_speaker_df=session_input_df[session_input_df['speaker']==speaker]\n",
    "        session_partner_df=session_input_df[session_input_df['speaker']==partner]\n",
    "    \n",
    "        speaker_sent_li=session_speaker_df['sentence'].tolist()\n",
    "        partner_sent_li=session_partner_df['sentence'].tolist()\n",
    "        \n",
    "        speaker_sent_li.sort()\n",
    "        partner_sent_li.sort()\n",
    "        print(\"speaker, speaker_sent_li\", speaker, speaker_sent_li)\n",
    "        print(\"partner, partner_sent_li\", partner, partner_sent_li)\n",
    "    \n",
    "        # dialogue_sent_li=speaker_sent_li+partner_sent_li\n",
    "        # dialogue_sent_li.sort()\n",
    "\n",
    "        speaker_adj_promixity=speaker_adj_promixity_dict[dyad]=[]\n",
    "        partner_adj_promixity=partner_adj_promixity_dict[dyad]=[]\n",
    "        speaker_other_promixity=speaker_other_promixity_dict[dyad]=[]\n",
    "        partner_other_promixity=partner_other_promixity_dict[dyad]=[]\n",
    "    \n",
    "        for id, (speaker_sent, partner_sent)in enumerate(zip(speaker_sent_li, partner_sent_li)):\n",
    "            # print(\"id, speaker_sent, partner_sent\", id, speaker_sent, partner_sent)\n",
    "            \n",
    "            speaker_feature=session_speaker_df.loc[session_speaker_df['sentence']==speaker_sent][feature].item()\n",
    "            partner_feature=session_partner_df.loc[session_partner_df['sentence']==partner_sent][feature].item()\n",
    "            # print(\"speaker_feature, partner_feature\", speaker_feature, partner_feature)\n",
    "    \n",
    "            adj_distance=abs(speaker_feature-partner_feature)\n",
    "    \n",
    "            sample_speaker_sent_li=speaker_sent_li.copy()\n",
    "            sample_partner_sent_li=partner_sent_li.copy()\n",
    "            sample_speaker_sent_li.remove(speaker_sent)\n",
    "            sample_partner_sent_li.remove(partner_sent)\n",
    "            \n",
    "            random10_speaker_features=[session_speaker_df.loc[session_speaker_df['sentence']==sent][feature].item() for sent in random.sample(sample_speaker_sent_li, 10)]\n",
    "            random10_partner_features=[session_partner_df.loc[session_partner_df['sentence']==sent][feature].item() for sent in random.sample(sample_partner_sent_li, 10)]\n",
    "            # print(\"random10_speaker_features\", random10_speaker_features)\n",
    "            # print(\"random10_partner_features\", random10_partner_features)\n",
    "            \n",
    "            other_speaker_distance=abs(speaker_feature-sum(random10_partner_features)/len(random10_partner_features))\n",
    "            other_partner_distance=abs(partner_feature-sum(random10_speaker_features)/len(random10_speaker_features))\n",
    "    \n",
    "            speaker_adj_promixity.append(adj_distance)\n",
    "            partner_adj_promixity.append(adj_distance)\n",
    "            speaker_other_promixity.append(other_speaker_distance)\n",
    "            partner_other_promixity.append(other_partner_distance)\n",
    "    \n",
    "            if id<len(speaker_sent_li)-1:\n",
    "                next_speaker_sent=speaker_sent_li[id+1]\n",
    "                # print(\"id, next_speaker_sent\", id, next_speaker_sent)\n",
    "                \n",
    "                next_speaker_feature=session_speaker_df.loc[session_speaker_df['sentence']==next_speaker_sent][feature].item()\n",
    "                next_adj_distance=abs(partner_feature-next_speaker_feature)\n",
    "                \n",
    "                next_sample_speaker_sent_li=sample_speaker_sent_li.copy()\n",
    "                next_sample_speaker_sent_li.remove(next_speaker_sent)\n",
    "                next_random10_speaker_features=[session_speaker_df.loc[session_speaker_df['sentence']==sent][feature].item() for sent in random.sample(next_sample_speaker_sent_li, 10)]\n",
    "                # print(\"next_random10_speaker_features\", next_random10_speaker_features)\n",
    "    \n",
    "                next_other_speaker_distance=abs(next_speaker_feature-sum(random10_partner_features)/len(random10_partner_features))\n",
    "                next_other_partner_distance=abs(partner_feature-sum(next_random10_speaker_features)/len(next_random10_speaker_features))\n",
    "    \n",
    "                speaker_adj_promixity.append(next_adj_distance)\n",
    "                partner_adj_promixity.append(next_adj_distance)\n",
    "                speaker_other_promixity.append(next_other_speaker_distance)\n",
    "                partner_other_promixity.append(next_other_partner_distance)\n",
    "            \n",
    "    \n",
    "    return speaker_adj_promixity_dict, partner_adj_promixity_dict, speaker_other_promixity_dict, partner_other_promixity_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c5c67d26-98f5-41e4-b4ce-b38a27575d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 7778\n",
      "speaker, speaker_sent_li 77 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 78 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7172\n",
      "speaker, speaker_sent_li 71 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 72 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6566\n",
      "speaker, speaker_sent_li 65 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 66 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7980\n",
      "speaker, speaker_sent_li 79 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 80 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7374\n",
      "speaker, speaker_sent_li 73 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 74 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6768\n",
      "speaker, speaker_sent_li 67 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 68 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6162\n",
      "speaker, speaker_sent_li 61 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 62 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 7576\n",
      "speaker, speaker_sent_li 75 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 76 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6970\n",
      "speaker, speaker_sent_li 69 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 70 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "processing 6364\n",
      "speaker, speaker_sent_li 63 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79]\n",
      "partner, partner_sent_li 64 [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80]\n",
      "[7778, 7172, 6566, 7980, 7374, 6768, 6162, 7576, 6970, 6364]\n",
      "79 [5.6482980000000005, 4.9919437, 5.6554848, 9.0413301, 8.0808039, 2.299321599999999, 4.40850195, 12.56293465, 9.0443544, 3.739404099999998, 5.518435799999999, 7.6930596000000016, 6.860419700000001, 4.608779899999998, 4.075171799999998, 4.444455499999998, 4.623426099999998, 3.7882631999999976, 6.939221659999998, 10.83201626, 6.9709, 5.213297099999998, 5.173030499999998, 4.2716707, 4.4991374, 5.079341699999999, 8.646041799999999, 5.4946629, 4.638120899999999, 6.2834608, 2.064288200000002, 2.0915152000000017, 3.6788708000000003, 2.0843780999999986, 2.6082098999999985, 4.558865299999999, 4.726096699999999, 5.6345423, 4.6084719, 4.2944403, 4.7801024000000005, 4.6763522, 6.410077299999999, 4.567289799999999, 5.5376917, 6.794363199999999, 6.197000299999999, 6.125831900000001, 4.323280500000001, 5.695952799999999, 6.771059599999999, 6.364209999999998, 5.517281699999998, 4.129342999999999, 6.165751899999998, 3.3868653999999996, 0.9091763999999998, 4.4767689, 7.677246030000001, 8.71998363, 7.5290038, 6.370649000000002, 10.031754150000001, 10.019540450000001, 2.8005070000000014, 3.121637399999999, 5.369296399999998, 6.888110599999999, 7.286722299999999, 4.935445199999998, 6.4133721999999995, 7.1903813, 6.9992619000000005, 8.5359573, 4.485913499999999, 3.7781349000000013, 5.7927091000000015, 4.791931499999999, 7.054289599999999]\n"
     ]
    }
   ],
   "source": [
    "speaker_adj_promixity_dict, partner_adj_promixity_dict, speaker_other_promixity_dict, partner_other_promixity_dict=compute_sentence_proximity(input_df, feature, session)\n",
    "dyad_keys=list(speaker_adj_promixity_dict.keys())\n",
    "print(dyad_keys)\n",
    "print(len(speaker_adj_promixity_dict[7778]), speaker_adj_promixity_dict[7778])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "232f74e6-2b9d-4d9a-9820-fc9f52f44201",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dyad_keys:\n",
    "    dyad_output_df=pd.DataFrame(list(zip(speaker_adj_promixity_dict[k], partner_adj_promixity_dict[k], speaker_other_promixity_dict[k], partner_other_promixity_dict[k])), \n",
    "                               columns=['speaker_adj_promixity', 'partner_adj_promixity', 'speaker_other_promixity', 'partner_other_promixity_dict'])\n",
    "    dyad_output_df.to_csv(\"{}_promixity.csv\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4c148-8647-421f-af18-2bb75bc8b830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
